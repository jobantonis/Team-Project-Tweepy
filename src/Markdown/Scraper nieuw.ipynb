{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter scraper COVID vaccine press conferences within the NetherlandsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a Twitter scraper in order to find out the sentiment towards to COVID-19 vaccine in the Netherlands, within the time frames surrounding the Dutch governments' Press Conferences. Herewith we want to measure the influence of the regulations regarding COVID-19 on the attitude people have towards COVID-19 vaccines.\n",
    "\n",
    "Our scraper results in a CSV file, containing the following entities:\n",
    "\n",
    "- Dutch tweets containing the following keywords: coronavaccin, corona_vaccin, covidvaccin, covid_vaccin, covid_vaccine, covidvaccine, coronavaccine, covid_vaccine\n",
    "- Specific dates the tweets were posted (around the persconferences (3 days before and 3 days after), datums + timeframe n.t.b. https://www.rijksoverheid.nl/onderwerpen/coronavirus-covid-19/coronavirus-beeld-en-video/videos-persconferenties)\n",
    "- Content of the tweets\n",
    "- User ID anoniem gemaakt d.m.v. ...\n",
    "\n",
    "##Datum nog veranderen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our code, we decided to use Selenium because Twitter is a dynamic webpage where we need to mimic scrolling like a Twitter user. Therefore, we first have to download and import some drivers and prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import selenium.webdriver   \n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we wrote a function in order to get multiple tweets. We used the .find()-function in order to find specific elements in the source code of the website. Below, we inserted 2 pictures, showing examples of how we found the specific elements (for username and comment). The '.text'-function makes sure we copy the text belonging to the element we were looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_data(card):\n",
    "    username = card.find_element_by_xpath('.//span').text\n",
    "    try:\n",
    "        handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text \n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    #text = comment + responding\n",
    "    reply_cnt = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    retweet_cnt = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    like_cnt = card.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    \n",
    "    tweet = {'username':username, \n",
    "             'handle':handle, \n",
    "             'date': postdate,\n",
    "             'comment': comment, \n",
    "             'responding': responding, \n",
    "             'reply_cnt':reply_cnt, \n",
    "             'retweet_cnt': retweet_cnt, \n",
    "             'like_cnt': like_cnt}\n",
    "    return(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Selenium, and therefore have to choose a browser in which we are going to open Twitter. We decided to use Chrome, so we set our Driver to Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we navigate Twitter to the page we want to scrape. We built in the dates we want to scrape and the keywords we are looking for. After this, we maximize the window to make sure we capture the whole page. Because loading the browser might take some time, we insert sleep(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://twitter.com/search?q=(vaccin%20OR%20vaccine%20OR%20coronavaccin%20OR%20corona_vaccin%20OR%20covidvaccin%20OR%20covid_vaccin%20OR%20corona_vaccine%20OR%20coronavaccine%20OR%20covidvaccine%20OR%20covid_vaccine)%20lang%3Anl%20until%3A2021-01-16%20since%3A2021-01-15&src=typed_query&f=live') \n",
    "driver.maximize_window() \n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually capture the tweets from the page, we wrote the following code. This code searches for all the tweets on the loaded page. It also uses the previously written code 'get_tweet_data(card)', which makes sure the data captured in the tweet is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function getting tweet data from page\n",
    "data = []\n",
    "def get_tweets():\n",
    "    page_cards = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]') #First div with which the scraper encounters a tweet\n",
    "    for card in page_cards[:len(page_cards)]:\n",
    "        tweet = get_tweet_data(card) \n",
    "        if tweet:\n",
    "            data.append(tweet)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Twitter is a dynamic website, we have to build a function which scrolls down the page automatically. 'driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')' makes sure the page is scrolled down until the site has to load again. Then we inserted sleep again, because it might take some time to load the page again. This makes sure our code will not continue while the page isn't fully loaded yet. Furthermore, we use 'last_position' and 'curr_position' to make sure a loop is build which scrolls down the page until the end. However, we are well aware that internet might lag and this could cause us to run out of the loop prematurily. Therefore, we allow a few scroll attempts before the code breaks (see: end of scroll region). Also, we have incorporated the previously written function 'get_tweets' into the code. Doing this makes sure the tweets are captured while scrolling. After this we also inserted some sleep time. Finally, we close the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function scroll code\n",
    "def auto_scroll():\n",
    "    scrolling = True\n",
    "    scroll_attempt = 0\n",
    "    last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    while True:\n",
    "        #check scroll position\n",
    "        get_tweets() #getting tweets while we scroll\n",
    "        sleep(2)\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(5) #giving program time to load before scraping\n",
    "        curr_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == curr_position: #breaks out of loop if current and last scroll positions are the same\n",
    "            scroll_attempt += 1 \n",
    "           \n",
    "            #end of scroll region\n",
    "            if scroll_attempt >= 3: \n",
    "                scrolling = False\n",
    "                break #Here: it makes sure the loop is ended\n",
    "            else:\n",
    "                sleep(2) # attempt another scroll\n",
    "        else:\n",
    "            last_position = curr_position \n",
    "            continue\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_scroll() # Run the scroll function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1398"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above code has run fully, we want to store the data into a CSV-file. Therefore we use the following code, which writes the data captured into a nicely structured CSV-file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DataCollection_Twitter.csv\", \"w\", newline=\"\", encoding='utf-8') as csv_file:\n",
    "  cols = ['username', \n",
    "             'handle', \n",
    "             'date',\n",
    "             'comment', \n",
    "             'responding', \n",
    "             'reply_cnt', \n",
    "             'retweet_cnt', \n",
    "             'like_cnt'] \n",
    "  writer = csv.DictWriter(csv_file, fieldnames=cols, restval='MISSING')\n",
    "  writer.writeheader()\n",
    "  writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to upload the data captured to a cloud. This is important, because this makes sure it is available, even if your computer crashes for example. For this code, Google Sheets is chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important!! Need to have the client file for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('client_secret.json', scope)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "spreadsheet = client.open('CSV-to-Google-Sheet')\n",
    "\n",
    "with open('DataCollection_Twitter.csv', 'r', encoding='latin-1') as file_obj:\n",
    "    content = file_obj.read()\n",
    "    client.import_csv(spreadsheet.id, data=content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
