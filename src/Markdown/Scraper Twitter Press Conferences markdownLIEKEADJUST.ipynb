{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter scraper COVID vaccine press conferences within the Netherlands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a Twitter scraper in order to find out the sentiment towards to COVID-19 vaccine in the Netherlands, within the time frames surrounding the Dutch governments' Press Conferences. Herewith we want to measure the influence of the regulations regarding COVID-19 on the attitude people have towards COVID-19 vaccines. \n",
    "\n",
    "Our scraper results in a CSV file, containing the following entities:\n",
    "- Dutch tweets containing the following keywords: *coronavaccin, corona_vaccin, covidvaccin, covid_vaccin, covid_vaccine, covidvaccine, coronavaccine, covid_vaccine*   \n",
    "- Specific dates the tweets were posted (around the persconferences (3 days before and 3 days after), *datums + timeframe n.t.b. https://www.rijksoverheid.nl/onderwerpen/coronavirus-covid-19/coronavirus-beeld-en-video/videos-persconferenties*) \n",
    "- Content of the tweets\n",
    "- User ID *anoniem gemaakt d.m.v. ...*\n",
    "\n",
    "##Datum nog veranderen!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our code, we decided to use Selenium because Twitter is a dynamic webpage where we need to mimic scrolling like a Twitter user.\n",
    "Therefore, we first have to download and import some drivers and prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import selenium.webdriver   \n",
    "from getpass import getpass\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we wrote a function in order to get multiple tweets. We used the .find()-function in order to find specific elements in the source code of the website. Below, we inserted 2 pictures, showing examples of how we found the specific elements (for username and comment). The '.text'-function makes sure we copy the text belonging to the element we were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jobantonis/Tremendously-awesome-repository/main/Screenshot%202021-03-11%20at%2017.17.12.png\" align=\"center\" width=60%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_data(card):\n",
    "    username = card.find_element_by_xpath('.//span').text\n",
    "    try:\n",
    "        handle = card.find_element_by_xpath('.//span[contains(text(), \"@\")]').text \n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        postdate = card.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    comment = card.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    responding = card.find_element_by_xpath('.//div[2]/div[2]/div[2]').text\n",
    "    #text = comment + responding\n",
    "    reply_cnt = card.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    retweet_cnt = card.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    like_cnt = card.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    \n",
    "    tweet = {'username':username, \n",
    "             'handle':handle, \n",
    "             'date': postdate,\n",
    "             'comment': comment, \n",
    "             'responding': responding, \n",
    "             'reply_cnt':reply_cnt, \n",
    "             'retweet_cnt': retweet_cnt, \n",
    "             'like_cnt': like_cnt}\n",
    "    return(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Selenium, and therefore have to choose a browser in which we are going to open Twitter. We decided to use Chrome, so we set our Driver to Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to log into Twitter. Therefore, we chose to use the URL containing the keywords and the specific dates we want to search for. Next, we open the Driver into Chrome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://twitter.com/search?q=(coronavaccin%20OR%20corona_vaccin%20OR%20covidvaccin%20OR%20covid_vaccin%20OR%20corona_vaccine%20OR%20coronavaccine%20OR%20covidvaccine%20OR%20covid_vaccine)%20lang%3Anl%20until%3A2020-05-18%20since%3A2020-05-12&src=typed_query&f=live') \n",
    "driver.maximize_window() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to fill the data with our input. Therefore, we create a list called 'data'. Furthermore, since it is a dynamic website which is accessed through scrolling through the site, we want to make sure we do not add the same tweets twice, therefore the function set(), called tweet_ids is added. Lastly, we added a function that executes the script and stops when we are at the last page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] #needed to fill a string with data\n",
    "tweet_ids = set() #Used in order to mitigate scraping duplicate tweets caused by possible increasing number of tweets per scroll \n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\") #For tracking scroll position, breaking out of loop if end is reached\n",
    "scrolling = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While scrolling, we want to make sure to capture our data from the tweets we encounter. After that, we actually begin to scroll down the pages. Since we are aware that there is a possibility that our internet laggs in between because of poor internet connection, we made sure our scraper tries to scroll twice before it is disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "while scrolling:\n",
    "    \n",
    "    page_cards = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]') #First div with which the scraper encounters a tweet\n",
    "    for card in page_cards[-len(page_cards):]: \n",
    "        tweet = get_tweet_data(card) \n",
    "       # if tweet not in data:\n",
    "        #    data.append(tweet)    \n",
    "        \n",
    " #       if tweet:\n",
    " #           tweet_id = tweet #make 1 tweet of the separate words used in the tweet\n",
    " #           if tweet_id not in tweet_ids: #Make sure you do not capture the same tweet twice, by only appending tweets that you haven't added before. \n",
    " #               tweet_ids.add(tweet_id) #to the set() function we add tweet_ids\n",
    " #               data.append(tweet) #add tweets to data list\n",
    "\n",
    "    scroll_attempt = 0 #used as sometimes due to lag scrolling will not register, so we allow for number of scroll attempts\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(2) #giving program time to load before scraping\n",
    "        curr_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if last_position == curr_position: #breaks out of loop if current and last scroll positions are the same\n",
    "            scroll_attempt += 1 \n",
    "            #2 attempts to check if it is the end of the page, since it is also possible that the scraper laggs because of poor internet connection.\n",
    "            if scroll_attempt >= 3: \n",
    "                scrolling = False\n",
    "                break\n",
    "            else: \n",
    "                sleep(2) \n",
    "        else:\n",
    "            last_position = curr_position #makes sure the loop ends\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the data to a CSV file, creating headers and writing data towards the file using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TESTANOUKcoronavaccin_tweetsuntil%3A2020-05-18%20since%3A2020-05-12.csv\", \"w\", newline=\"\", encoding='utf-8') as csv_file:\n",
    "  cols = ['username', \n",
    "             'handle', \n",
    "             'date',\n",
    "             'comment', \n",
    "             'responding', \n",
    "             'reply_cnt', \n",
    "             'retweet_cnt', \n",
    "             'like_cnt'] \n",
    "  writer = csv.DictWriter(csv_file, fieldnames=cols, restval='MISSING')\n",
    "  writer.writeheader()\n",
    "  writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
